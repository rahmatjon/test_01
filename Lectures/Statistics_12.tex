\documentclass[aspectratio=169]{beamer}
%\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{cmap}
\usetheme{Boadilla}

\newcommand{\Ki}{\mathscr{K}}
\newcommand{\D}{\mathscr{D}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bes}{\begin{equation*}}
\newcommand{\ees}{\end{equation*}}

\makeatletter
\setbeamertemplate{footline}{%
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.92\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}\insertshorttitle
    \end{beamercolorbox}%
  }%
  \begin{beamercolorbox}[wd=.08\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}%
    \usebeamertemplate{page number in head/foot}%
    \hspace*{2ex} 
  \end{beamercolorbox}
  \vskip0pt%
}
\makeatother

\title{\bf Урок 12. Регрессия (часть 1)}
\author{{\bf Хакимов Р.И. + ChatGPT}}
 \date[\today]{}

\begin{document}
\begin{frame}
\titlepage
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf Простая линейная регрессия} — это метод статистического анализа, который используется для моделирования и анализа линейной зависимости между двумя переменными: одной независимой (или объясняющей) переменной и одной зависимой (или откликовой) переменной.\\
Цель простого линейного регрессийного анализа — построить линейную модель, которая наилучшим образом описывает связь между этими переменными.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf  Модель простой линейной регрессии}\\
Модель простой линейной регрессии имеет следующий вид:
\[
Y = \beta_0 + \beta_1 X + \epsilon
\]
где:\\
\quad \( Y \) — зависимая переменная (отклик).\\
\quad \( X \) — независимая переменная (объясняющая).\\
\quad \( \beta_0 \) — свободный член (пересечение линии с осью Y).\\
\quad \( \beta_1 \) — коэффициент наклона (показывает, насколько изменяется Y при изменении X на единицу).\\
\quad \( \epsilon \) — случайная ошибка (различие между предсказанным и фактическим значением Y).
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf Процесс построения модели}\\
{\it 1. Сбор данных.} Соберите данные для зависимой и независимой переменных.\\
{\it 2. Построение диаграммы рассеяния.} Постройте диаграмму рассеяния для визуализации взаимосвязи между переменными. Это поможет определить, насколько хорошо данные могут быть представлены линейной моделью.\\
{\it 3. Расчет коэффициентов регрессии.} Используйте метод наименьших квадратов для оценки коэффициентов \( \beta_0 \) и \( \beta_1 \). Метод наименьших квадратов минимизирует сумму квадратов отклонений предсказанных значений от фактических данных.\\
{\it 4. Оценка модели.} Проверьте качество модели с помощью статистических метрик, таких как коэффициент детерминации \( R^2 \), стандартная ошибка регрессии, и тестирование значимости коэффициентов.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf Формулы для коэффициентов}\\
1. Коэффициент наклона (\( \beta_1 \)):
\[
\beta_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
\]
2. Свободный член (\( \beta_0 \)):
\[
\beta_0 = \bar{Y} - \beta_1 \bar{X}
\]
где \( \bar{X} \) и \( \bar{Y} \) — средние значения переменных X и Y соответственно.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf Интерпретация результатов}\\
Коэффициент наклона (\( \beta_1 \)) показывает, на сколько единиц изменяется зависимая переменная \( Y \) при увеличении независимой переменной \( X \) на одну единицу. Если \( \beta_1 \) положителен, связь между переменными положительная; если отрицателен — связь отрицательная.\\
2. Свободный член (\( \beta_0 \)) - это значение зависимой переменной \( Y \), когда независимая переменная \( X \) равна нулю. Это может быть интерпретировано как начальная точка на оси Y.\\
3. Коэффициент детерминации (\( R^2 \)) показывает, какая доля вариации зависимой переменной объясняется моделью. Значение \( R^2 \) варьируется от 0 до 1. Чем ближе \( R^2 \) к 1, тем лучше модель объясняет вариацию данных.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
{\bf Коэффициент детерминации \( R^2 \) (R-squared)} — это статистическая метрика, которая показывает, насколько хорошо модель регрессии объясняет изменчивость зависимой переменной. Значение \( R^2 \) варьируется от 0 до 1. Чем ближе \( R^2 \) к 1, тем лучше модель описывает данные.\\
Формула для вычисления \( R^2 \) следующая:
\[
R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
\]
где:\\
- \( SS_{\text{res}} \) — сумма квадратов остатков (residual sum of squares), которая вычисляется как:
\[
SS_{\text{res}} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\] 
где \( y_i \) — наблюдаемые значения зависимой переменной, \( \hat{y}_i \) — предсказанные моделью значения.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
- \( SS_{\text{tot}} \) — полная сумма квадратов (total sum of squares), которая выражается как:
\[
SS_{\text{tot}} = \sum_{i=1}^{n} (y_i - \bar{y})^2
\]
где \( \bar{y} \) — среднее значение наблюдаемых данных.
\newline\\
Таким образом, коэффициент детерминации \( R^2 \) показывает долю общей вариации зависимой переменной, которая объясняется моделью. Если \( R^2 = 1 \), то модель точно предсказывает все наблюдаемые значения. Если \( R^2 = 0 \), модель не объясняет изменчивость данных вообще.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия}
- \( SS_{\text{tot}} \) — полная сумма квадратов (total sum of squares), которая выражается как:
\[
SS_{\text{tot}} = \sum_{i=1}^{n} (y_i - \bar{y})^2
\]
где \( \bar{y} \) — среднее значение наблюдаемых данных.
\newline\\
Таким образом, коэффициент детерминации \( R^2 \) показывает долю общей вариации зависимой переменной, которая объясняется моделью. Если \( R^2 = 1 \), то модель точно предсказывает все наблюдаемые значения. Если \( R^2 = 0 \), модель не объясняет изменчивость данных вообще.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 1}
Рассмотрим пример расчета коэффициентов простой линейной регрессии. Пусть у нас есть следующие данные о зависимости переменной \( y \) от переменной \( x \):
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 \( x \) & \( y \)\\ 
 \hline
 1 & 2\\
 \hline
 2 & 3\\
 \hline
 3 & 4\\
 \hline
 4 & 5\\
 \hline
 5 & 6\\
 \hline
\end{tabular}
\end{center}
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 1}
Мы хотим найти коэффициенты простой линейной регрессии вида:
\[
y = \beta_0 + \beta_1 x
\]
{\bf Шаг 1:} Найдем средние значения \( \bar{x} \) и \( \bar{y} \)
\[
\bar{x} = \frac{1 + 2 + 3 + 4 + 5}{5} = 3
\]
\[
\bar{y} = \frac{2 + 3 + 5 + 4 + 6}{5} = 4
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 1}
{\bf Шаг 2:} Найдем коэффициент наклона \( \beta_1 \)\\
Коэффициент наклона \( \beta_1 \) вычисляется по формуле:
\[
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\]
Вычислим сумму \( \sum (x_i - \bar{x})(y_i - \bar{y}) \):
  \[
  (1 - 3)(2 - 4) + (2 - 3)(3 - 4) + (3 - 3)(5 - 4) + (4 - 3)(4 - 4) + (5 - 3)(6 - 4)
  \]
  \[
  = (-2)(-2) + (-1)(-1) + (0)(1) + (1)(0) + (2)(2)
  \]
  \[
  = 4 + 1 + 0 + 0 + 4 = 9
  \]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 1}
Вычислим сумму \( \sum (x_i - \bar{x})^2 \):\\
  \[
  (1 - 3)^2 + (2 - 3)^2 + (3 - 3)^2 + (4 - 3)^2 + (5 - 3)^2
  \]
  \[
  = (-2)^2 + (-1)^2 + (0)^2 + (1)^2 + (2)^2
  \]
  \[
  = 4 + 1 + 0 + 1 + 4 = 10
  \]
Теперь можно найти \( \beta_1 \):
\[
\beta_1 = \frac{9}{10} = 0.9
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 1}
{\bf Шаг 3:} Найдем свободный коэффициент \( \beta_0 \)\\
Свободный коэффициент \( \beta_0 \) вычисляется по формуле:
\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]
Подставим значения:
\[
\beta_0 = 4 - 0.9 \times 3 = 4 - 2.7 = 1.3
\]
{\bf Итоговая модель}\\
Итак, уравнение регрессии имеет вид:
\[
y = 1.3 + 0.9x
\]
Теперь можно использовать это уравнение для предсказания значений \( y \) на основе значений \( x \).
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 2}
Рассмотрим пример расчета коэффициентов простой линейной регрессии. Пусть у нас есть небольшие данные о зависимости роста человека \( y \) от его веса \( x \):
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Вес (x), кг & Рост (y), см \\ 
 \hline
 60 & 155\\
 \hline
 65 & 160\\
 \hline
 70 & 168\\
 \hline
 75 & 175\\
 \hline
 80 & 180\\
 \hline
\end{tabular}
\end{center}
Задача заключается в нахождении коэффициентов \( \beta_0 \) (свободный член) и \( \beta_1 \) (наклон прямой) для уравнения линейной регрессии:
\[
y = \beta_0 + \beta_1 x
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 2}
{\bf Формулы для расчета коэффициентов}\\
1. Коэффициент наклона \( \beta_1 \):
\[
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\]
2. Свободный член \( \beta_0 \):
\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]
где:\\
\quad \( \bar{x} \) — среднее значение \( x \),\\
\quad \( \bar{y} \) — среднее значение \( y \),\\
\quad \( x_i \) и \( y_i \) — значения каждой пары данных.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 2}
{\bf Шаг 1.} Найдем средние значения \( \bar{x} \) и \( \bar{y} \):
\[
\bar{x} = \frac{60 + 65 + 70 + 75 + 80}{5} = 70
\]
\[
\bar{y} = \frac{155 + 160 + 168 + 175 + 180}{5} = 167.6
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 2}
{\bf Шаг 2.} Найдем \( \beta_1 \):\\
Найдём числитель и знаменатель для формулы \( \beta_1 \):
\[
\sum (x_i - \bar{x})(y_i - \bar{y}) = (60 - 70)(155 - 167.6) + (65 - 70)(160 - 167.6) +
\]
\[
+ (70 - 70)(168 - 167.6) + (75 - 70)(175 - 167.6) + (80 - 70)(180 - 167.6) =
\]
\[
= (-10)(-12.6) + (-5)(-7.6) + (0)(0.4) + (5)(7.4) + (10)(12.4) =
\]
\[
= 126 + 38 + 0 + 37 + 124 = 325
\]
\[
\sum (x_i - \bar{x})^2 = (60 - 70)^2 + (65 - 70)^2 + (70 - 70)^2 + (75 - 70)^2 + (80 - 70)^2
\]
\[
= (-10)^2 + (-5)^2 + (0)^2 + (5)^2 + (10)^2 = 100 + 25 + 0 + 25 + 100 = 250
\]
Теперь найдём \( \beta_1 \):
\[
\beta_1 = \frac{325}{250} = 1.3
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Простая линейная регрессия. Пример 2}
{\bf Шаг 3.} Найдем \( \beta_0 \):
\[
\beta_0 = \bar{y} - \beta_1 \bar{x} = 167.6 - 1.3 \times 70 = 167.6 - 91 = 76.6
\]
{\bf Шаг 4.} Итоговое уравнение регрессии:\\
Подставив коэффициенты, получаем уравнение линейной регрессии:
\[
y = 76.6 + 1.3x
\]
Это уравнение позволяет предсказывать рост человека на основании его веса. Например, если вес человека \( x = 68 \) кг, предсказанный рост будет:
\[
y = 76.6 + 1.3 \times 68 = 76.6 + 88.4 = 165 \text{ см}
\]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия}
{\bf Множественная линейная регрессия} — это метод статистического анализа, который используется для моделирования зависимости одной зависимой переменной (или отклика) от нескольких независимых переменных (или предикторов). Модель множественной линейной регрессии может быть выражена следующим уравнением:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \varepsilon
\]
где:\\
\quad \(Y\) — зависимая переменная,\\
\quad \(X_1, X_2, ..., X_n\) — независимые переменные (факторы),\\
\quad \(\beta_0\) — свободный член (константа),\\
\quad \(\beta_1, \beta_2, ..., \beta_n\) — коэффициенты регрессии, показывающие вклад каждой независимой переменной в предсказание зависимой переменной,\\
\quad \(\varepsilon\) — ошибка модели (остаток), отражающая разницу между фактическими и предсказанными значениями \(Y\).
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия}
{\bf Основные предпосылки множественной линейной регрессии:}\\
{\it 1. Линейность:} связь между зависимой и независимыми переменными должна быть линейной.\\
{\it 2. Независимость:} остатки (ошибки модели) должны быть независимы.\\
{\it 3. Нормальность остатков:} ошибки модели должны быть нормально распределены.\\
{\it 4. Гомоскедастичность:} дисперсия остатков должна быть одинаковой для всех значений независимых переменных.\\
{\it 5. Отсутствие мультиколлинеарности:} независимые переменные не должны сильно коррелировать между собой.
\newline\\
Множественная линейная регрессия позволяет оценить, насколько сильно каждый из факторов влияет на зависимую переменную и как изменяется прогнозируемая переменная при изменении независимых переменных.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия}
Коэффициенты множественной линейной регрессии можно рассчитать с помощью метода наименьших квадратов. Формула для расчета коэффициентов регрессии в множественной линейной регрессии (векторные формы) выглядит следующим образом:
\[
\beta = (X^T X)^{-1} X^T y
\]
где:\\
- \(\beta\) — вектор коэффициентов регрессии,\\
- \(X\) — матрица признаков (независимых переменных), где каждая строка соответствует наблюдению, а каждый столбец — переменной. Первой колонкой обычно добавляется столбец единиц для учета свободного члена \(\beta_0\),\\
- \(y\) — вектор зависимой переменной.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия}
{\bf Шаги для расчета коэффициентов:}\\
{\it 1. Подготовка данных:} Создайте матрицу \(X\), добавив в нее столбец единиц для свободного члена \(\beta_0\).\\
{\it 2. Вычисление матрицы \(X^T X\):} Транспонируйте матрицу \(X\) и умножьте на саму матрицу \(X\).\\
{\it 3. Вычисление обратной матрицы:} Найдите обратную матрицу для результата, полученного на предыдущем шаге.\\
{\it 4. Вычисление \(X^T y\):} Умножьте транспонированную матрицу \(X\) на вектор \(y\).\\
{\it 5. Умножение:} Умножьте обратную матрицу на результат, полученный на предыдущем шаге.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия. Пример}
Предположим, у нас есть следующие данные:
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Площадь (x1) & Количество комнат (x2) & Цена (y) \\ 
 \hline
 50 & 2 & 15000\\
 \hline
 70 & 3 & 20000\\
 \hline
 100 & 4 & 30000\\
 \hline
\end{tabular}
\end{center}
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Множественная линейная регрессия. Пример}
{\bf Шаги для расчета коэффициентов:}\\
1. Создание матрицы \(X\):
   \[
   X = \begin{bmatrix}
   1 & 50 & 2 \\
   1 & 70 & 3 \\
   1 & 100 & 4
   \end{bmatrix}
   \]
2. Создание вектора \(y\):
   \[
   y = \begin{bmatrix}
   150000 \\
   200000 \\
   300000
   \end{bmatrix}
   \]
3. Вычисление:\\
\quad \(X^T X\)\\
\quad \((X^T X)^{-1}\)\\
\quad \(X^T y\)\\
\quad Наконец, \(\beta = (X^T X)^{-1} X^T y\)
\end{frame}

\end{document}