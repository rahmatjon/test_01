\documentclass[aspectratio=169]{beamer}
%\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{cmap}
\usetheme{Boadilla}

\newcommand{\Ki}{\mathscr{K}}
\newcommand{\D}{\mathscr{D}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bes}{\begin{equation*}}
\newcommand{\ees}{\end{equation*}}

\makeatletter
\setbeamertemplate{footline}{%
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.92\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
      \usebeamerfont{title in head/foot}\insertshorttitle
    \end{beamercolorbox}%
  }%
  \begin{beamercolorbox}[wd=.08\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}%
    \usebeamertemplate{page number in head/foot}%
    \hspace*{2ex} 
  \end{beamercolorbox}
  \vskip0pt%
}
\makeatother

\title{\bf Урок 13. Регрессия (часть 2)}
\author{{\bf Хакимов Р.И. + ChatGPT}}
 \date[\today]{}

\begin{document}
\begin{frame}
\titlepage
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Модель нелинейной регрессии}
Модель нелинейной регрессии может быть представлена в виде:
\[
Y = f(X_1, X_2, \ldots, X_p; \beta) + \epsilon
\]
где:\\
- \( Y \) — зависимая переменная.\\
- \( X_1, X_2, \ldots, X_p \) — независимые переменные.\\
- \( f(\cdot) \) — нелинейная функция, определяющая зависимость между переменными.\\
- \( \beta \) — вектор параметров модели.\\
- \( \epsilon \) — случайная ошибка.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Примеры нелинейных функций}
{\bf 1. Экспоненциальная функция:}
   \[
   Y = \beta_0 e^{\beta_1 X}
   \]
{\bf 2. Логарифмическая функция:}
   \[
   Y = \beta_0 + \beta_1 \log(X)
   \]
{\bf 3. Полиномиальная функция (высшего порядка):}
   \[
   Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \beta_p X^p
   \]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Примеры нелинейных функций}
{\bf 4. Гиперболическая функция:}
   \[
   Y = \frac{\beta_0}{\beta_1 + X}
   \]
{\bf 5. Сигмойдальная функция (логистическая регрессия):}
   \[
   Y = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
   \]
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Модель нелинейной регрессии}
{\bf Преимущества:}\\
\quad {\it Гибкость:} Нелинейные модели могут лучше описывать сложные зависимости между переменными.\\
\quad {\it Точность:} Может предоставить более точные предсказания, если связь действительно нелинейная.
\newline\\
{\bf Недостатки:}\\
\quad {\it Сложность:} Нелинейные модели могут быть сложными для интерпретации и требуют больше вычислительных ресурсов.\\
\quad {\it Перегрузка:} Модели могут переобучаться на данных, если слишком сложные функции используются.\\
\quad {\it Итеративные методы:} Оценка параметров требует итеративных методов, которые могут быть чувствительны к начальным условиям и локальным минимумам.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Методы выбора признаков на основе корреляции}
Когда мы работаем с множеством признаков, некоторые из них могут быть слабо коррелированы с целевой переменной или сильно коррелированы друг с другом. Это может негативно сказаться на качестве модели, так как избыточная информация (высокая корреляция между признаками) может привести к многоколлинеарности. Чтобы избежать этого, часто применяют методы выбора признаков на основе корреляции.\\
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Методы выбора признаков на основе корреляции}
{\bf 1. Корреляция признаков с целевой переменной:} Признаки, которые слабо коррелированы с целевой переменной, могут быть исключены из модели, так как они не предоставляют полезной информации для предсказания целевой переменной. С другой стороны, признаки с высокой корреляцией (в положительном или отрицательном направлении) более важны и предпочтительны.\\
{\bf 2. Корреляция между признаками:} Признаки, которые сильно коррелированы между собой, могут дублировать информацию. В таких случаях можно оставить только один из сильно коррелированных признаков, чтобы снизить избыточность в данных и избежать многоколлинеарности.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Многоколлинеарность}
{\bf Многоколлинеарность (или мультиколлинеарность)} в математической статистике и эконометрике — это ситуация, когда в множественной линейной регрессии одна из независимых переменных может быть представлена как линейная комбинация других независимых переменных с высокой точностью. Это приводит к тому, что переменные оказываются сильно взаимосвязанными, что может создавать проблемы при оценке коэффициентов регрессии.\\
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Многоколлинеарность}
Основные проблемы, связанные с многоколлинеарностью:\\
{\bf 1. Неустойчивость оценок:} коэффициенты регрессии могут быть сильно чувствительны к малейшим изменениям в данных, что делает их оценки менее надежными.\\
{\bf 2. Проблемы с интерпретацией:} если независимые переменные сильно коррелируют, то сложно понять, какой именно переменной приписывать влияние на зависимую переменную.\\
{\bf 3. Высокие стандартные ошибки:} это снижает статистическую значимость коэффициентов, что может приводить к неверным выводам о значимости переменных.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Многоколлинеарность}
Для диагностики многоколлинеарности часто используют:\\
{\bf - Коэффициент детерминации \( R^2 \)} между переменными.\\
{\bf - Фактор инфляции дисперсии (VIF)}: если \( VIF \) для переменной превышает 10, это может указывать на многоколлинеарность.\\
Методы борьбы с многоколлинеарностью:\\
1. Исключение одной из взаимосвязанных переменных.\\
2. Применение методов регуляризации, таких как ридж-регрессия (Ridge regression), которые штрафуют большие коэффициенты и снижают влияние многоколлинеарности.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Матрица корреляции}
{\bf Матрица корреляции} — это квадратная таблица, которая показывает коэффициенты корреляции между всеми парами признаков в наборе данных. Каждый элемент матрицы представляет собой коэффициент корреляции Пирсона для двух признаков. Значения коэффициента корреляции находятся в диапазоне от -1 до 1:\\
- Значение близкое к 1 означает сильную положительную корреляцию (признаки изменяются в одном направлении).\\
- Значение близкое к -1 означает сильную отрицательную корреляцию (признаки изменяются в противоположных направлениях).\\
- Значение близкое к 0 означает отсутствие корреляции.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Матрица корреляции}
Матрица корреляции помогает:\\
- Определить взаимосвязи между признаками.\\
- Найти признаки с сильной взаимной корреляцией, чтобы исключить лишние признаки.\\
- Выявить важные признаки для целевой переменной.\\
Пример матрицы корреляции для набора признаков может выглядеть так:\\
\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
  & Признак 1 & Признак 2 & Признак 3 & Целевая переменная \\ 
 \hline
 Признак 1 & 1.0 & 0.8 & -0.5 & 0.7\\
 \hline
 Признак 2 & 0.8 & 1.0 & -0.4 & 0.6\\
 \hline
 Признак 3 & -0.5 & -0.4 & 1.0 & -0.3\\
 \hline
 Целевая переменная & 0.7 & 0.6 & -0.3 & 1.0\\
 \hline
\end{tabular}
\end{center}
Здесь можно видеть, что Признак 1 и Признак 2 сильно коррелируют между собой (0.8), что может указывать на необходимость исключения одного из них из модели.
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Матрица корреляции}
{\bf Основные шаги выбора признаков на основе корреляции:}\\
1. Построение матрицы корреляции.\\
2. Оценка корреляции каждого признака с целевой переменной.\\
3. Исключение признаков с низкой корреляцией с целевой переменной.\\
4. Исключение признаков с высокой взаимной корреляцией.
\newline\\
Эти шаги помогают упростить модель, сократить количество признаков и улучшить интерпретируемость результатов.
\end{frame}

\end{document}